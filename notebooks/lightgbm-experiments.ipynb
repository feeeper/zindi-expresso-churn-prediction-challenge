{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c264df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f475eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df_train = pd.read_csv('./data/Train_folds.zip')\n",
    "df_test=  pd.read_csv('./data/Test.zip')\n",
    "submission = pd.read_csv('./data/SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b21e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b656fe52973c438c90492287c57c4d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b2cd6ce05c4851a62bf400c3cecbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2696b955a664b7389e241c1e107313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "top pack:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>REGION</th>\n",
       "      <th>TENURE</th>\n",
       "      <th>MONTANT</th>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <th>FREQUENCE</th>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <th>ON_NET</th>\n",
       "      <th>...</th>\n",
       "      <th>IS_TOP_PACK_OTHER</th>\n",
       "      <th>IS_TOP_PACK_Data_50F_30MB_24H</th>\n",
       "      <th>IS_TOP_PACK_Data_3000F_10GB_30d</th>\n",
       "      <th>IS_TOP_PACK_Data_500F_2GB_24H</th>\n",
       "      <th>IS_TOP_PACK_Data_300F_100MB_2d</th>\n",
       "      <th>IS_TOP_PACK_Data_1000F_5GB_7d</th>\n",
       "      <th>IS_TOP_PACK_Data_150F_SPPackage1_24H</th>\n",
       "      <th>IS_ALL_NET</th>\n",
       "      <th>TARGET_ENC_REGION</th>\n",
       "      <th>TARGET_ENC_TENURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000bfd7d50f01092811bc0c8d7b0d6fe7c3596</td>\n",
       "      <td>FATICK</td>\n",
       "      <td>4</td>\n",
       "      <td>4250.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4251.000000</td>\n",
       "      <td>1417.00000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014196</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000cb4a5d760de88fecb38e2f71b7bec52e834</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>3</td>\n",
       "      <td>5067.795106</td>\n",
       "      <td>11.478503</td>\n",
       "      <td>5012.078888</td>\n",
       "      <td>1670.69839</td>\n",
       "      <td>14.031420</td>\n",
       "      <td>3705.837220</td>\n",
       "      <td>267.946292</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447987</td>\n",
       "      <td>0.270341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001654a9d9f96303d9969d0a4a851714a4bb57</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>4</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>340.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3317.913239</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447987</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001dd6fa45f7ba044bd5d84937be464ce78ac2</td>\n",
       "      <td>DAKAR</td>\n",
       "      <td>4</td>\n",
       "      <td>13500.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13502.000000</td>\n",
       "      <td>4501.00000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>43804.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000028d9e13a595abe061f9b58f3d76ab907850f</td>\n",
       "      <td>DAKAR</td>\n",
       "      <td>4</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>328.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3317.913239</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154043</th>\n",
       "      <td>ffffe85215ddc71a84f95af0afb0deeea90e6967</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>4</td>\n",
       "      <td>5551.330713</td>\n",
       "      <td>11.535439</td>\n",
       "      <td>5531.026497</td>\n",
       "      <td>1843.68163</td>\n",
       "      <td>13.982138</td>\n",
       "      <td>3317.913239</td>\n",
       "      <td>278.320853</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447987</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154044</th>\n",
       "      <td>ffffeaaa9289cdba0ac000f0ab4b48f4aa74ed15</td>\n",
       "      <td>THIES</td>\n",
       "      <td>4</td>\n",
       "      <td>6100.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5800.000000</td>\n",
       "      <td>1933.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154045</th>\n",
       "      <td>fffff172fda1b4bb38a95385951908bb92379809</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>4</td>\n",
       "      <td>5551.330713</td>\n",
       "      <td>11.535439</td>\n",
       "      <td>5531.026497</td>\n",
       "      <td>1843.68163</td>\n",
       "      <td>13.982138</td>\n",
       "      <td>3317.913239</td>\n",
       "      <td>278.320853</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447987</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154046</th>\n",
       "      <td>fffff5911296937a37f09a37a549da2e0dad6dbb</td>\n",
       "      <td>THIES</td>\n",
       "      <td>4</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7120.000000</td>\n",
       "      <td>2373.00000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3317.913239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016301</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154047</th>\n",
       "      <td>fffff6dbff1508ea2bfe814e5ab2729ce6b788c2</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>4</td>\n",
       "      <td>5551.330713</td>\n",
       "      <td>11.535439</td>\n",
       "      <td>5531.026497</td>\n",
       "      <td>1843.68163</td>\n",
       "      <td>13.982138</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>278.320853</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447987</td>\n",
       "      <td>0.183531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154048 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_id  REGION  TENURE  \\\n",
       "0        00000bfd7d50f01092811bc0c8d7b0d6fe7c3596  FATICK       4   \n",
       "1        00000cb4a5d760de88fecb38e2f71b7bec52e834   OTHER       3   \n",
       "2        00001654a9d9f96303d9969d0a4a851714a4bb57   OTHER       4   \n",
       "3        00001dd6fa45f7ba044bd5d84937be464ce78ac2   DAKAR       4   \n",
       "4        000028d9e13a595abe061f9b58f3d76ab907850f   DAKAR       4   \n",
       "...                                           ...     ...     ...   \n",
       "2154043  ffffe85215ddc71a84f95af0afb0deeea90e6967   OTHER       4   \n",
       "2154044  ffffeaaa9289cdba0ac000f0ab4b48f4aa74ed15   THIES       4   \n",
       "2154045  fffff172fda1b4bb38a95385951908bb92379809   OTHER       4   \n",
       "2154046  fffff5911296937a37f09a37a549da2e0dad6dbb   THIES       4   \n",
       "2154047  fffff6dbff1508ea2bfe814e5ab2729ce6b788c2   OTHER       4   \n",
       "\n",
       "              MONTANT  FREQUENCE_RECH       REVENUE  ARPU_SEGMENT  FREQUENCE  \\\n",
       "0         4250.000000       15.000000   4251.000000    1417.00000  17.000000   \n",
       "1         5067.795106       11.478503   5012.078888    1670.69839  14.031420   \n",
       "2         3600.000000        2.000000   1020.000000     340.00000   2.000000   \n",
       "3        13500.000000       15.000000  13502.000000    4501.00000  18.000000   \n",
       "4         1000.000000        1.000000    985.000000     328.00000   1.000000   \n",
       "...               ...             ...           ...           ...        ...   \n",
       "2154043   5551.330713       11.535439   5531.026497    1843.68163  13.982138   \n",
       "2154044   6100.000000       15.000000   5800.000000    1933.00000  15.000000   \n",
       "2154045   5551.330713       11.535439   5531.026497    1843.68163  13.982138   \n",
       "2154046  10000.000000       11.000000   7120.000000    2373.00000  13.000000   \n",
       "2154047   5551.330713       11.535439   5531.026497    1843.68163  13.982138   \n",
       "\n",
       "          DATA_VOLUME      ON_NET  ...  IS_TOP_PACK_OTHER  \\\n",
       "0            4.000000  388.000000  ...                  0   \n",
       "1         3705.837220  267.946292  ...                  1   \n",
       "2         3317.913239   90.000000  ...                  0   \n",
       "3        43804.000000   41.000000  ...                  0   \n",
       "4         3317.913239   39.000000  ...                  0   \n",
       "...               ...         ...  ...                ...   \n",
       "2154043   3317.913239  278.320853  ...                  1   \n",
       "2154044    621.000000   26.000000  ...                  0   \n",
       "2154045   3317.913239  278.320853  ...                  1   \n",
       "2154046   3317.913239    0.000000  ...                  0   \n",
       "2154047      2.000000  278.320853  ...                  1   \n",
       "\n",
       "         IS_TOP_PACK_Data_50F_30MB_24H  IS_TOP_PACK_Data_3000F_10GB_30d  \\\n",
       "0                                    0                                0   \n",
       "1                                    0                                0   \n",
       "2                                    0                                0   \n",
       "3                                    0                                0   \n",
       "4                                    0                                0   \n",
       "...                                ...                              ...   \n",
       "2154043                              0                                0   \n",
       "2154044                              0                                0   \n",
       "2154045                              0                                0   \n",
       "2154046                              0                                0   \n",
       "2154047                              0                                0   \n",
       "\n",
       "         IS_TOP_PACK_Data_500F_2GB_24H IS_TOP_PACK_Data_300F_100MB_2d  \\\n",
       "0                                    0                              0   \n",
       "1                                    0                              0   \n",
       "2                                    0                              0   \n",
       "3                                    0                              0   \n",
       "4                                    0                              0   \n",
       "...                                ...                            ...   \n",
       "2154043                              0                              0   \n",
       "2154044                              0                              0   \n",
       "2154045                              0                              0   \n",
       "2154046                              0                              0   \n",
       "2154047                              0                              0   \n",
       "\n",
       "         IS_TOP_PACK_Data_1000F_5GB_7d IS_TOP_PACK_Data_150F_SPPackage1_24H  \\\n",
       "0                                    0                                    0   \n",
       "1                                    0                                    0   \n",
       "2                                    0                                    0   \n",
       "3                                    1                                    0   \n",
       "4                                    0                                    0   \n",
       "...                                ...                                  ...   \n",
       "2154043                              0                                    0   \n",
       "2154044                              0                                    0   \n",
       "2154045                              0                                    0   \n",
       "2154046                              0                                    0   \n",
       "2154047                              0                                    0   \n",
       "\n",
       "         IS_ALL_NET  TARGET_ENC_REGION  TARGET_ENC_TENURE  \n",
       "0                 0           0.014196           0.183531  \n",
       "1                 0           0.447987           0.270341  \n",
       "2                 0           0.447987           0.183531  \n",
       "3                 0           0.019235           0.183531  \n",
       "4                 0           0.019235           0.183531  \n",
       "...             ...                ...                ...  \n",
       "2154043           0           0.447987           0.183531  \n",
       "2154044           0           0.016301           0.183531  \n",
       "2154045           0           0.447987           0.183531  \n",
       "2154046           1           0.016301           0.183531  \n",
       "2154047           0           0.447987           0.183531  \n",
       "\n",
       "[2154048 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_train.copy()\n",
    "test = df_test.copy()\n",
    "\n",
    "cat_cols = [\n",
    "    'REGION',\n",
    "#    'TENURE',\n",
    "#     'TOP_PACK'\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    'MONTANT',\n",
    "    'FREQUENCE_RECH',\n",
    "    'REVENUE',\n",
    "    'ARPU_SEGMENT',\n",
    "    'FREQUENCE',\n",
    "    'DATA_VOLUME',\n",
    "    'ON_NET', \n",
    "    'ORANGE',\n",
    "    'TIGO',\n",
    "    'ZONE1',\n",
    "    'ZONE2',\n",
    "    'REGULARITY',\n",
    "    'FREQ_TOP_PACK',\n",
    "]\n",
    "\n",
    "target = 'CHURN'\n",
    "\n",
    "mapping = {\n",
    "    'D 3-6 month': 1,\n",
    "    'E 6-9 month': 2,\n",
    "    'F 9-12 month': 3,\n",
    "    'G 12-15 month': 4,\n",
    "    'H 15-18 month': 5,\n",
    "    'I 18-21 month': 6,\n",
    "    'J 21-24 month': 7,\n",
    "    'K > 24 month': 8,\n",
    "}\n",
    "\n",
    "mapping = {\n",
    "    'D 3-6 month': 1,\n",
    "    'E 6-9 month': 1,\n",
    "    'F 9-12 month': 2,\n",
    "    'G 12-15 month': 2,\n",
    "    'H 15-18 month': 3,\n",
    "    'I 18-21 month': 3,\n",
    "    'J 21-24 month': 4,\n",
    "    'K > 24 month': 4,\n",
    "}\n",
    "\n",
    "train['TENURE'] = train['TENURE'].map(mapping) \n",
    "test['TENURE'] = test['TENURE'].map(mapping)\n",
    "\n",
    "train['REGION'] = train['REGION'].fillna('OTHER')\n",
    "test['REGION'] = test['REGION'].fillna('OTHER')\n",
    "\n",
    "train['TOP_PACK'] = train['TOP_PACK'].fillna('OTHER')\n",
    "test['TOP_PACK'] = test['TOP_PACK'].fillna('OTHER')\n",
    "\n",
    "agg_by_tenure = pd.read_csv('./data/agg_by_tenure.csv')\n",
    "agg_by_tenure_dict = {x['TENURE']: x for x in agg_by_tenure.to_dict('records')}\n",
    "\n",
    "agg_by_region = pd.read_csv('./data/agg_by_region.csv')\n",
    "agg_by_region_dict = {x['REGION']: x for x in agg_by_region.to_dict('records')}\n",
    "\n",
    "\n",
    "def by_region(x: pd.Series, col: str, how: str) -> float:\n",
    "    return agg_by_region_dict[x['REGION']][f'{col}_{how}'] if np.isnan(x[f'{col}']) else x[f'{col}']\n",
    "\n",
    "def by_tenure(x: pd.Series, col: str, how: str) -> float:\n",
    "    return agg_by_tenure_dict[x['TENURE']][f'{col}_{how}'] if np.isnan(x[f'{col}']) else x[f'{col}']\n",
    "\n",
    "train_merged_with_tenure = pd.merge(train, agg_by_tenure, left_on='TENURE', right_on='TENURE', how='left')\n",
    "test_merged_with_tenure = pd.merge(test, agg_by_tenure, left_on='TENURE', right_on='TENURE', how='left')\n",
    "\n",
    "train_merged_with_region = pd.merge(train, agg_by_region, left_on='REGION', right_on='REGION', how='left')\n",
    "test_merged_with_region = pd.merge(test, agg_by_region, left_on='REGION', right_on='REGION', how='left')\n",
    "\n",
    "for col in tqdm(num_cols):\n",
    "    col_mean = train[col].mean()\n",
    "    # train[col] = train[col].fillna(col_mean)\n",
    "    # test[col] = test[col].fillna(col_mean)\n",
    "    # train[col] = train.apply(lambda x: by_tenure(x, col, 'mean'), axis=1) \n",
    "    # test[col] = test.apply(lambda x: by_tenure(x, col, 'mean'), axis=1)\n",
    "    train[col] = np.where(train[col].isnull(), train_merged_with_tenure[f'{col}_mean'], train[col])\n",
    "    test[col] = np.where(test[col].isnull(), test_merged_with_tenure[f'{col}_mean'], test[col])\n",
    "\n",
    "for col in tqdm(['DATA_VOLUME', 'ON_NET']): # судя по весам фич только они влияют\n",
    "#     train[f'{col}_FNAN_REGION_mean'] = train.apply(lambda x: by_region(x, col, 'mean'), axis=1)\n",
    "#     test[f'{col}_FNAN_REGION_mean'] = test.apply(lambda x: by_region(x, col, 'mean'), axis=1)\n",
    "    train[f'{col}_FNAN_REGION_mean'] = np.where(train[col].isnull(), train_merged_with_region[f'{col}_mean'], train[col])\n",
    "    test[f'{col}_FNAN_REGION_mean'] = np.where(test[col].isnull(), test_merged_with_region[f'{col}_mean'], test[col])\n",
    "\n",
    "train['IS_UNLIMITED'] = train['TOP_PACK'].apply(lambda x: 1 if 'unlimited' in x.lower() else 0)\n",
    "test['IS_UNLIMITED'] = test['TOP_PACK'].apply(lambda x: 1 if 'unlimited' in x.lower() else 0)\n",
    "\n",
    "for tp in tqdm(['OTHER', 'Data:50F=30MB_24H', 'Data:3000F=10GB,30d', 'Data:500F=2GB,24H', 'Data:300F=100MB,2d', 'Data:1000F=5GB,7d', 'Data:150F=SPPackage1,24H'], desc='top pack'):\n",
    "    col_name = re.sub('[:=,]', '_', tp)\n",
    "    train[f'IS_TOP_PACK_{col_name}'] = train['TOP_PACK'].apply(lambda x: 1 if x == tp else 0)\n",
    "    test[f'IS_TOP_PACK_{col_name}'] = test['TOP_PACK'].apply(lambda x: 1 if x == tp else 0)\n",
    "\n",
    "train['IS_ALL_NET'] = train['TOP_PACK'].apply(lambda x: 1 if 'allnet' in x.replace('-', '').lower() else 0)\n",
    "test['IS_ALL_NET'] = test['TOP_PACK'].apply(lambda x: 1 if 'allnet' in x.replace('-', '').lower() else 0)\n",
    "\n",
    "\n",
    "# target encoding\n",
    "te_region = train.groupby('REGION').agg({'CHURN': 'mean'}).reset_index()\n",
    "te_region.columns = ['REGION', 'TARGET_ENC_REGION']\n",
    "\n",
    "te_tenure = train.groupby('TENURE').agg({'CHURN': 'mean'}).reset_index()\n",
    "te_tenure.columns = ['TENURE', 'TARGET_ENC_TENURE']\n",
    "\n",
    "train = pd.merge(train, \n",
    "                 te_region,\n",
    "                 left_on='REGION',\n",
    "                 right_on='REGION',\n",
    "                 how='left',\n",
    "                 suffixes=('_x', '_TARGET_ENC_REGION'))\n",
    "test =  pd.merge(test, \n",
    "                 te_region,\n",
    "                 left_on='REGION',\n",
    "                 right_on='REGION',\n",
    "                 how='left',\n",
    "                 suffixes=('_x', '_TARGET_ENC_REGION'))\n",
    "\n",
    "train = pd.merge(train, \n",
    "                 te_tenure,\n",
    "                 left_on='TENURE',\n",
    "                 right_on='TENURE',\n",
    "                 how='left',\n",
    "                 suffixes=('_x', '_TARGET_ENC_TENURE'))\n",
    "test =  pd.merge(test, \n",
    "                 te_tenure,\n",
    "                 left_on='TENURE',\n",
    "                 right_on='TENURE',\n",
    "                 how='left',\n",
    "                 suffixes=('_x', '_TARGET_ENC_TENURE'))\n",
    "\n",
    "\n",
    "# не нужон - по весам фич\n",
    "#     train[f'{col}_FNAN_REGION_median'] = train.apply(lambda x: by_region(x, col, 'median'), axis=1)\n",
    "#     test[f'{col}_FNAN_REGION_median'] = test.apply(lambda x: by_region(x, col, 'median'), axis=1)\n",
    "    \n",
    "#     train[f'{col}_FNAN_TENURE_mean'] = train.apply(lambda x: by_tenure(x, col, 'mean'), axis=1)\n",
    "#     test[f'{col}_FNAN_TENURE_mean'] = test.apply(lambda x: by_tenure(x, col, 'mean'), axis=1)\n",
    "\n",
    "#     train[f'{col}_FNAN_TENURE_median'] = train.apply(lambda x: by_tenure(x, col, 'median'), axis=1)\n",
    "#     test[f'{col}_FNAN_TENURE_median'] = test.apply(lambda x: by_tenure(x, col, 'median'), axis=1)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bc22114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'REGION', 'TENURE', 'MONTANT', 'FREQUENCE_RECH', 'REVENUE',\n",
       "       'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO',\n",
       "       'ZONE1', 'ZONE2', 'MRG', 'REGULARITY', 'TOP_PACK', 'FREQ_TOP_PACK',\n",
       "       'CHURN', 'kfold', 'DATA_VOLUME_FNAN_REGION_mean',\n",
       "       'ON_NET_FNAN_REGION_mean', 'IS_UNLIMITED', 'IS_TOP_PACK_OTHER',\n",
       "       'IS_TOP_PACK_Data_50F_30MB_24H', 'IS_TOP_PACK_Data_3000F_10GB_30d',\n",
       "       'IS_TOP_PACK_Data_500F_2GB_24H', 'IS_TOP_PACK_Data_300F_100MB_2d',\n",
       "       'IS_TOP_PACK_Data_1000F_5GB_7d', 'IS_TOP_PACK_Data_150F_SPPackage1_24H',\n",
       "       'IS_ALL_NET', 'TARGET_ENC_REGION', 'TARGET_ENC_TENURE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1401309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1286afc096984130a9670b29a0c3b92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7979141115063451\n",
      "1 0.7980816317132172\n",
      "2 0.7966558287337555\n",
      "3 0.7985057742891991\n",
      "4 0.7957348496077602\n",
      "0.7973784391700554 0.0010272158066749583\n"
     ]
    }
   ],
   "source": [
    "# train_copy = train.copy()\n",
    "# test_copy = test.copy()\n",
    "\n",
    "# useful_cols = [col for col in train_copy.columns if col not in set(['user_id', \n",
    "#                                                                     'MRG',\n",
    "#                                                                     'TOP_PACK', \n",
    "#                                                                     'CHURN',\n",
    "#                                                                     'kfold'])]\n",
    "\n",
    "# final_predictions = []\n",
    "# scores = []\n",
    "\n",
    "# minmax_scaler_cols = ['DATA_VOLUME', 'ON_NET']\n",
    "# scaler = MinMaxScaler()\n",
    "# train_copy[minmax_scaler_cols] = scaler.fit_transform(train_copy[minmax_scaler_cols])\n",
    "# test_copy[minmax_scaler_cols] = scaler.transform(test_copy[minmax_scaler_cols])\n",
    "\n",
    "# standard_scaler_cols = [col for col in train_copy.columns if col not in set(['user_id', 'MRG', 'TOP_PACK', 'REGION', 'DATA_VOLUME', 'ON_NET', 'kfold', 'CHURN'])]\n",
    "# scaler = StandardScaler()\n",
    "# train_copy[standard_scaler_cols] = scaler.fit_transform(train_copy[standard_scaler_cols])\n",
    "# test_copy[standard_scaler_cols] = scaler.transform(test_copy[standard_scaler_cols])\n",
    "\n",
    "# # poly features\n",
    "# numerical_cols = [\n",
    "#     'DATA_VOLUME',\n",
    "#     'ON_NET',\n",
    "# # 'MONTANT',\n",
    "# # 'FREQUENCE_RECH',\n",
    "# # 'REVENUE',\n",
    "# # 'ARPU_SEGMENT',\n",
    "# # 'FREQUENCE',\n",
    "#  'ORANGE',\n",
    "#  'TIGO',\n",
    "# # 'ZONE1',\n",
    "# # 'ZONE2',\n",
    "# # 'REGULARITY',\n",
    "# ]\n",
    "# poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "# train_poly = poly.fit_transform(train_copy[numerical_cols])\n",
    "# test_poly = poly.fit_transform(test_copy[numerical_cols])\n",
    "\n",
    "# poly_columns = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\n",
    "# df_poly = pd.DataFrame(train_poly, columns=poly_columns)\n",
    "# df_test_poly = pd.DataFrame(test_poly, columns=poly_columns)\n",
    "\n",
    "# train_copy = pd.concat([train_copy, df_poly], axis=1)\n",
    "# test_copy = pd.concat([test_copy, df_test_poly], axis=1)\n",
    "\n",
    "# useful_cols += poly_columns\n",
    "\n",
    "# for cat_col in cat_cols:\n",
    "#     encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "#     unique_values = train_copy[cat_col].unique()\n",
    "\n",
    "#     one_hot_encoded_cols = [f'{cat_col}_{i}' for i in range(len(unique_values))]\n",
    "    \n",
    "#     ohe_df = pd.DataFrame(encoder.fit_transform(train_copy[[cat_col]]).toarray(), columns=one_hot_encoded_cols)\n",
    "#     ohe_df.index = train_copy.index\n",
    "#     train_copy = train_copy.drop(cat_col, axis=1)\n",
    "#     train_copy = pd.concat([train_copy, ohe_df], axis=1)        \n",
    "#     print(f'[{cat_col}] xtrain transformed')\n",
    "\n",
    "#     ohe_df = pd.DataFrame(encoder.transform(test_copy[[cat_col]]).toarray(), columns=one_hot_encoded_cols)\n",
    "#     ohe_df.index = test_copy.index\n",
    "#     test_copy = test_copy.drop(cat_col, axis=1)\n",
    "#     test_copy = pd.concat([test_copy, ohe_df], axis=1)\n",
    "#     print(f'[{cat_col}] xtest transformed')\n",
    "    \n",
    "#     useful_cols += one_hot_encoded_cols\n",
    "#     useful_cols.remove(cat_col)\n",
    "\n",
    "final_predictions = []\n",
    "scores = []\n",
    "\n",
    "target = 'CHURN'\n",
    "\n",
    "for fold in tqdm(range(5), 'folds'):\n",
    "    xtrain = train_copy[train_copy['kfold'] != fold][useful_cols]\n",
    "    ytrain = train_copy[train_copy['kfold'] != fold][target]\n",
    "    \n",
    "    xvalid = train_copy[train['kfold'] == fold][useful_cols]\n",
    "    yvalid = train_copy[train['kfold'] == fold][target]\n",
    "\n",
    "    xtest = test_copy[useful_cols]\n",
    "\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=1000,\n",
    "#         random_state=42,\n",
    "#     )\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=7000,\n",
    "        random_state=42,\n",
    "        **{\n",
    "            'learning_rate': 0.023262668329845724,\n",
    "            'reg_lambda': 8.946573985262771e-05,\n",
    "            'reg_alpha': 8.609876549670105e-06,\n",
    "            'subsample': 0.3971516543340211,\n",
    "            'colsample_bytree': 0.32107183361408465,\n",
    "            'max_depth': 1\n",
    "        }\n",
    "    )\n",
    "    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n",
    "    \n",
    "    preds_valid = model.predict(xvalid)\n",
    "    test_preds = model.predict(xtest)\n",
    "    final_predictions.append(test_preds)\n",
    "    score = roc_auc_score(yvalid, preds_valid)\n",
    "    scores.append(score)\n",
    "    print(fold, score)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))\n",
    "\n",
    "# 0.7696186855892555 0.00043098014634677365 fillna через REGION\n",
    "# 0.7694106138854746 0.000588962476667447 fillna через TENURE\n",
    "# 0.7701952731313458 0.0006247026357018513 fillna через mean по колонке\n",
    "# 0.769901864294696 0.000610047154013374 fillna через mean по колонке & ohe for region\n",
    "# 0.769901864294696 0.000610047154013374 fillna через mean по колонке & ohe for region + StandardScaler for Tenure (same as prev)\n",
    "# 0.770010208315598 0.0005090535431930928 fillna через mean по колонке & ohe for region & tenure [private: 0.853185019447002]\n",
    "# 0.770008209799155 0.0005120012284348264 fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds\n",
    "# 0.7701761776716778 0.0007820978999800708 fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten [private 0.862163874472749]\n",
    "# 0.7690741534134118 0.0008824035170153271 fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten + изменил фолды (!= и ==)\n",
    "# 0.7690825678568933 0.0008789684650258934  fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten + изменил фолды (!= и ==)\n",
    "# 0.7698245780943753 0.000724931679586648 lgb - fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten + изменил фолды (!= и ==)\n",
    "# 0.7694104784482177 0.0013440865822247955 lgb - fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten + изменил фолды (!= и ==) + optuna params [0.760814445393041]\n",
    "# 0.7696374297117283 0.0010216807303939667 lgb - fillna через mean по колонке & ohe for region & tenure for whole train and test datasets & StScaler on whole ds + target encoding by reg & ten + изменил фолды (!= и ==) + num cols log1\n",
    "\n",
    "# 0.768700101552386 0.0010140633965792358 lgb + ohe for region + w/o tenure\n",
    "# 0.7913442391232364 0.0009072160914931118 lgb + ohe for region + w/o tenure + region fillna other\n",
    "# 0.7915846676604728 0.000801936503179641 lgb + ohe for region + 3 mon tenure + region fillna other\n",
    "# 0.7915156853124097 0.000992492975077104 lgb + ohe for region + 12 mon tenure + region fillna other\n",
    "# 0.7917672724064511 0.0008976053140583513 lgb + ohe for region + 6 mon tenure + region fillna other\n",
    "# 0.7917653651343787 0.0007855762351865283 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num\n",
    "# 0.7918252728749173 0.0008695062597926666 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num + StSc whole ds\n",
    "# 0.7914845860192667 0.0010262092086456543 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num + MinMaxSc whole ds\n",
    "# 0.7919582623977296 0.0008726366301281235 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other)\n",
    "# 0.7913209118545288 0.0010748640472510074 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean region + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + DATA_VOLUME_FNAN_REGION_mean & ON_NET_FNAN_REGION_mean\n",
    "# 0.792256907136914 0.0010776815408567177  lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) [0.795227047440307]\n",
    "# 0.7916985286519639 0.0009710673470586245 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + TOP_PACK (scaled)\n",
    "# 0.7915685818170114 0.0008091780308036663 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + TOP_PACK (ohe)\n",
    "# 0.7915685818170114 0.0008091780308036663 (ok) lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + TOP_PACK (fillna other + ohe)\n",
    "# 0.7916914872303236 0.0007342967833541975 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + TOP_PACK (fillna other + ohe) + IS_UNLIMITED\n",
    "# 0.7919668892976947 0.0009393373192820406 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED\n",
    "# 0.7916230116088253 0.0009519019905584034 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN\n",
    "# 0.791875399477451 0.0009425233968772518 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN + is_all_net\n",
    "# [86890349-b4ba-4f56-932b-d9445ff4d4f5] 0.791692177101185 0.0009334681681677956 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN + is_all_net + top of top_pack\n",
    "# [b624b4d3-3845-4d17-8648-694d2c7de821] 0.7919930343268513 0.0008727219454338667 lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN + is_all_net + top of top_pack + te by reg & ten [0.795441938635621]\n",
    "# [94863b76-a609-4faa-9294-141455a761bc] 0.7922070116912214 0.0009209575881898701  lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN + is_all_net + top of top_pack + te by reg & ten + poly 'DATA_VOLUME','ON_NET','ORANGE', 'TIGO', [0.795539929288825]\n",
    "# [79fc566c-55c8-4a59-9a7e-69e3c6789bed] 0.7973784391700554 0.0010272158066749583  lgb + ohe for region + 6 mon tenure + region fillna other + fillna num by mean tenure + MinMaxSc(DATA_VOLUME,ON_NET) + StSc(other) + IS_UNLIMITED + IS_TOP_PACK_NAN + is_all_net + top of top_pack + te by reg & ten + poly 'DATA_VOLUME','ON_NET','ORANGE', 'TIGO' + optuna []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "588d7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "\n",
    "submission = pd.read_csv('./data/SampleSubmission.csv')\n",
    "submission.CHURN = preds\n",
    "# submission.to_csv(\"./data/submission-lgb-5-folds-1000-est-42-rs-TENURE-4-REGION-fillna-Other-REG-OHE-StScaler&MinMixSc-fillna-by-mean-Tenure.csv\", index=False)\n",
    "submission.to_csv('./data/submission-94863b76-a609-4faa-9294-141455a761bc.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict(zip(model.feature_name_, model.feature_importances_)).items(), key=lambda x: -x[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e05be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from collections import defaultdict\n",
    "\n",
    "# display(Markdown('**_some_ markdown** and an [internal reference](use/format/markdown)!'))\n",
    "\n",
    "top_pack_values = train['TOP_PACK'].value_counts().index # train['TOP_PACK'].unique()\n",
    "percents = defaultdict(list)\n",
    "for tp in tqdm(top_pack_values):\n",
    "    tmp = train[train['TOP_PACK'] == tp][['TOP_PACK', 'CHURN']]\n",
    "    total = len(tmp)\n",
    "    display(Markdown(f'**{tp} ({total})**'))\n",
    "    vc = tmp.value_counts()\n",
    "    try:\n",
    "        percents[tp].append(total)\n",
    "        try:\n",
    "            percents[tp].append(vc[0])\n",
    "        except:\n",
    "            percents[tp].append(0)\n",
    "        \n",
    "        try:\n",
    "            percents[tp].append(vc[1])\n",
    "        except:\n",
    "            percents[tp].append(0)\n",
    "        \n",
    "        percents[tp].append(percents[tp][-1]/percents[tp][-2] if percents[tp][-2] != 0 else 0)\n",
    "        display(Markdown(f'0: {vc[0]}\\t1: {vc[1]}\\t({vc[1]/vc[0]*100}%)'))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_copy = train.copy()\n",
    "# test_copy = test.copy()\n",
    "\n",
    "# useful_cols = [col for col in train_copy.columns if col not in set(['user_id', \n",
    "#                                                                     'MRG',\n",
    "#                                                                     'TOP_PACK', \n",
    "#                                                                     'CHURN',\n",
    "#                                                                     'kfold'])]\n",
    "\n",
    "# final_predictions = []\n",
    "# scores = []\n",
    "\n",
    "# minmax_scaler_cols = ['DATA_VOLUME', 'ON_NET']\n",
    "# scaler = MinMaxScaler()\n",
    "# train_copy[minmax_scaler_cols] = scaler.fit_transform(train_copy[minmax_scaler_cols])\n",
    "# test_copy[minmax_scaler_cols] = scaler.transform(test_copy[minmax_scaler_cols])\n",
    "\n",
    "# standard_scaler_cols = [col for col in train_copy.columns if col not in set(['user_id', 'MRG', 'TOP_PACK', 'REGION', 'DATA_VOLUME', 'ON_NET', 'kfold', 'CHURN'])]\n",
    "# scaler = StandardScaler()\n",
    "# train_copy[standard_scaler_cols] = scaler.fit_transform(train_copy[standard_scaler_cols])\n",
    "# test_copy[standard_scaler_cols] = scaler.transform(test_copy[standard_scaler_cols])\n",
    "\n",
    "# # poly features\n",
    "# numerical_cols = [\n",
    "#     'DATA_VOLUME',\n",
    "#     'ON_NET',\n",
    "# # 'MONTANT',\n",
    "# # 'FREQUENCE_RECH',\n",
    "# # 'REVENUE',\n",
    "# # 'ARPU_SEGMENT',\n",
    "# # 'FREQUENCE',\n",
    "#  'ORANGE',\n",
    "#  'TIGO',\n",
    "# # 'ZONE1',\n",
    "# # 'ZONE2',\n",
    "# # 'REGULARITY',\n",
    "# ]\n",
    "# poly = PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\n",
    "# train_poly = poly.fit_transform(train_copy[numerical_cols])\n",
    "# test_poly = poly.fit_transform(test_copy[numerical_cols])\n",
    "\n",
    "# poly_columns = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\n",
    "# df_poly = pd.DataFrame(train_poly, columns=poly_columns)\n",
    "# df_test_poly = pd.DataFrame(test_poly, columns=poly_columns)\n",
    "\n",
    "# train_copy = pd.concat([train_copy, df_poly], axis=1)\n",
    "# test_copy = pd.concat([test_copy, df_test_poly], axis=1)\n",
    "\n",
    "# useful_cols += poly_columns\n",
    "\n",
    "# for cat_col in cat_cols:\n",
    "#     encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "#     unique_values = train_copy[cat_col].unique()\n",
    "\n",
    "#     one_hot_encoded_cols = [f'{cat_col}_{i}' for i in range(len(unique_values))]\n",
    "    \n",
    "#     ohe_df = pd.DataFrame(encoder.fit_transform(train_copy[[cat_col]]).toarray(), columns=one_hot_encoded_cols)\n",
    "#     ohe_df.index = train_copy.index\n",
    "#     train_copy = train_copy.drop(cat_col, axis=1)\n",
    "#     train_copy = pd.concat([train_copy, ohe_df], axis=1)        \n",
    "#     print(f'[{cat_col}] xtrain transformed')\n",
    "\n",
    "#     ohe_df = pd.DataFrame(encoder.transform(test_copy[[cat_col]]).toarray(), columns=one_hot_encoded_cols)\n",
    "#     ohe_df.index = test_copy.index\n",
    "#     test_copy = test_copy.drop(cat_col, axis=1)\n",
    "#     test_copy = pd.concat([test_copy, ohe_df], axis=1)\n",
    "#     print(f'[{cat_col}] xtest transformed')\n",
    "    \n",
    "#     useful_cols += one_hot_encoded_cols\n",
    "#     useful_cols.remove(cat_col)\n",
    "\n",
    "# final_predictions = []\n",
    "# scores = []\n",
    "\n",
    "# target = 'CHURN'\n",
    "\n",
    "# for fold in tqdm(range(5), 'folds'):\n",
    "#     xtrain = train_copy[train_copy['kfold'] != fold][useful_cols]\n",
    "#     ytrain = train_copy[train_copy['kfold'] != fold][target]\n",
    "    \n",
    "#     xvalid = train_copy[train['kfold'] == fold][useful_cols]\n",
    "#     yvalid = train_copy[train['kfold'] == fold][target]\n",
    "\n",
    "#     xtest = test_copy[useful_cols]\n",
    "\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=1000,\n",
    "#         random_state=42,\n",
    "#     )\n",
    "#     model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n",
    "    \n",
    "#     preds_valid = model.predict(xvalid)\n",
    "#     test_preds = model.predict(xtest)\n",
    "#     final_predictions.append(test_preds)\n",
    "#     score = roc_auc_score(yvalid, preds_valid)\n",
    "#     scores.append(score)\n",
    "    \n",
    "def run(trial):\n",
    "    fold = 0\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n",
    "    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n",
    "\n",
    "    xtrain = train_copy[train_copy.kfold != fold][useful_cols]\n",
    "    xvalid = train_copy[train_copy.kfold == fold][useful_cols]\n",
    "\n",
    "    ytrain = train_copy[train_copy.kfold != fold]['CHURN']\n",
    "    yvalid = train_copy[train_copy.kfold == fold]['CHURN']\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=7000,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n",
    "    preds_valid = model.predict(xvalid)\n",
    "    score = roc_auc_score(yvalid, preds_valid)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(run, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0550ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9055b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "046a05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def run_xgb_optuna(trial):\n",
    "    fold = 0\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n",
    "    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n",
    "    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n",
    "\n",
    "    xtrain = train_copy[train_copy.kfold != fold][useful_cols]\n",
    "    xvalid = train_copy[train_copy.kfold == fold][useful_cols]\n",
    "\n",
    "    ytrain = train_copy[train_copy.kfold != fold]['CHURN']\n",
    "    yvalid = train_copy[train_copy.kfold == fold]['CHURN']\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=7000,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_lambda=reg_lambda,\n",
    "        reg_alpha=reg_alpha,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        max_depth=max_depth,\n",
    "    )\n",
    "    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n",
    "    preds_valid = model.predict(xvalid)\n",
    "    score = roc_auc_score(yvalid, preds_valid)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da111d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 01:27:10,203]\u001b[0m A new study created in memory with name: no-name-665d20d9-61dc-460b-b848-db457a3a50f4\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:27:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68149\n",
      "[1000]\tvalidation_0-logloss:0.25379\n",
      "[2000]\tvalidation_0-logloss:0.25319\n",
      "[3000]\tvalidation_0-logloss:0.25299\n",
      "[4000]\tvalidation_0-logloss:0.25289\n",
      "[5000]\tvalidation_0-logloss:0.25277\n",
      "[6000]\tvalidation_0-logloss:0.25271\n",
      "[6999]\tvalidation_0-logloss:0.25265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 03:01:33,334]\u001b[0m Trial 0 finished with value: 0.7954320714993549 and parameters: {'learning_rate': 0.018907594739771442, 'reg_lambda': 0.08026291453520452, 'reg_alpha': 4.836923798243186e-08, 'subsample': 0.3938610342201946, 'colsample_bytree': 0.5068260136551423, 'max_depth': 2}. Best is trial 0 with value: 0.7954320714993549.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:01:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.58525\n",
      "[354]\tvalidation_0-logloss:0.25377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 03:13:47,418]\u001b[0m Trial 1 finished with value: 0.7941735109926839 and parameters: {'learning_rate': 0.17664658289943563, 'reg_lambda': 0.0004752770266892279, 'reg_alpha': 0.057120664597860804, 'subsample': 0.1751141830068263, 'colsample_bytree': 0.9339586497539522, 'max_depth': 5}. Best is trial 1 with value: 0.7941735109926839.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.65560\n",
      "[675]\tvalidation_0-logloss:0.25177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 03:47:16,069]\u001b[0m Trial 2 finished with value: 0.7927349963264996 and parameters: {'learning_rate': 0.05746047770798967, 'reg_lambda': 0.9292007699948006, 'reg_alpha': 3.164644611685049e-05, 'subsample': 0.7202855688350371, 'colsample_bytree': 0.902286297275291, 'max_depth': 6}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68712\n",
      "[1000]\tvalidation_0-logloss:0.25731\n",
      "[2000]\tvalidation_0-logloss:0.25600\n",
      "[3000]\tvalidation_0-logloss:0.25566\n",
      "[4000]\tvalidation_0-logloss:0.25551\n",
      "[5000]\tvalidation_0-logloss:0.25541\n",
      "[6000]\tvalidation_0-logloss:0.25537\n",
      "[6999]\tvalidation_0-logloss:0.25534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 05:00:24,304]\u001b[0m Trial 3 finished with value: 0.7983046424621432 and parameters: {'learning_rate': 0.010041245886806825, 'reg_lambda': 5.016153632857239, 'reg_alpha': 0.033617106775221055, 'subsample': 0.17184616927494392, 'colsample_bytree': 0.7555828926003856, 'max_depth': 1}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:00:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.60036\n",
      "[502]\tvalidation_0-logloss:0.25233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 05:18:04,196]\u001b[0m Trial 4 finished with value: 0.7936248700025921 and parameters: {'learning_rate': 0.1500612420693415, 'reg_lambda': 0.00019797127807751045, 'reg_alpha': 0.0002144005512273368, 'subsample': 0.6433310356635589, 'colsample_bytree': 0.7375072273334858, 'max_depth': 5}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:18:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.67750\n",
      "[1000]\tvalidation_0-logloss:0.25242\n",
      "[2000]\tvalidation_0-logloss:0.25209\n",
      "[3000]\tvalidation_0-logloss:0.25203\n",
      "[3067]\tvalidation_0-logloss:0.25203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 06:18:09,835]\u001b[0m Trial 5 finished with value: 0.7939694871305556 and parameters: {'learning_rate': 0.024781603552755917, 'reg_lambda': 3.3064961948530582, 'reg_alpha': 2.6114243493137353e-05, 'subsample': 0.47137682114260615, 'colsample_bytree': 0.18761539248810677, 'max_depth': 5}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.61438\n",
      "[607]\tvalidation_0-logloss:0.25216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 06:39:16,995]\u001b[0m Trial 6 finished with value: 0.7931673728683969 and parameters: {'learning_rate': 0.1258680215182785, 'reg_lambda': 0.008220970922931697, 'reg_alpha': 7.158816638550568e-08, 'subsample': 0.8226281967433343, 'colsample_bytree': 0.6920058027399136, 'max_depth': 5}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:39:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68379\n",
      "[1000]\tvalidation_0-logloss:0.25457\n",
      "[2000]\tvalidation_0-logloss:0.25341\n",
      "[3000]\tvalidation_0-logloss:0.25309\n",
      "[4000]\tvalidation_0-logloss:0.25293\n",
      "[5000]\tvalidation_0-logloss:0.25278\n",
      "[6000]\tvalidation_0-logloss:0.25272\n",
      "[6999]\tvalidation_0-logloss:0.25266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 08:00:22,070]\u001b[0m Trial 7 finished with value: 0.7950448515675372 and parameters: {'learning_rate': 0.015410942593942921, 'reg_lambda': 0.1749725455845635, 'reg_alpha': 60.7302175039379, 'subsample': 0.45616195291634365, 'colsample_bytree': 0.3813325494194799, 'max_depth': 2}. Best is trial 2 with value: 0.7927349963264996.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.65328\n",
      "[751]\tvalidation_0-logloss:0.25183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 08:24:43,836]\u001b[0m Trial 8 finished with value: 0.7923984926440721 and parameters: {'learning_rate': 0.06492027716998955, 'reg_lambda': 9.574177772304593e-05, 'reg_alpha': 0.394796743340963, 'subsample': 0.7396657363246338, 'colsample_bytree': 0.6147305790208815, 'max_depth': 6}. Best is trial 8 with value: 0.7923984926440721.\u001b[0m\n",
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.67260\n",
      "[1000]\tvalidation_0-logloss:0.25206\n",
      "[2000]\tvalidation_0-logloss:0.25193\n",
      "[2985]\tvalidation_0-logloss:0.25193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 09:31:08,580]\u001b[0m Trial 9 finished with value: 0.7942637781369735 and parameters: {'learning_rate': 0.03132064404372426, 'reg_lambda': 1.1176152679148211e-06, 'reg_alpha': 0.0003264218372696942, 'subsample': 0.5930600752785693, 'colsample_bytree': 0.9085543387896339, 'max_depth': 4}. Best is trial 8 with value: 0.7923984926440721.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06492027716998955, 'reg_lambda': 9.574177772304593e-05, 'reg_alpha': 0.394796743340963, 'subsample': 0.7396657363246338, 'colsample_bytree': 0.6147305790208815, 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "xgb_study = optuna.create_study(direction=\"minimize\")\n",
    "xgb_study.optimize(run_xgb_optuna, n_trials=10)\n",
    "print(xgb_study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd1d7ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ce878141a24ef3a57f86d5586f86f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:06:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0 0.7923984926440721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:26:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1 0.7921119240890732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:42:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 0.7917168672681814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:58:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3 0.7929651662720968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "4 0.7908389036514595\n",
      "0.7920062707849765 0.000711474747631675\n"
     ]
    }
   ],
   "source": [
    "# xgb_params = {\n",
    "#     'learning_rate': 0.012077288295042267,\n",
    "#     'reg_lambda': 0.0006873465542426026,\n",
    "#     'reg_alpha': 1.3212946403386152e-06,\n",
    "#     'subsample': 0.7581724871412163,\n",
    "#     'colsample_bytree': 0.609065197494544,\n",
    "#     'max_depth': 2\n",
    "# }\n",
    "xgb_params = {'learning_rate': 0.06492027716998955, 'reg_lambda': 9.574177772304593e-05, 'reg_alpha': 0.394796743340963, 'subsample': 0.7396657363246338, 'colsample_bytree': 0.6147305790208815, 'max_depth': 6}\n",
    "\n",
    "final_predictions = []\n",
    "scores = []\n",
    "\n",
    "target = 'CHURN'\n",
    "\n",
    "for fold in tqdm(range(5), 'folds'):\n",
    "    xtrain = train_copy[train_copy['kfold'] != fold][useful_cols]\n",
    "    ytrain = train_copy[train_copy['kfold'] != fold][target]\n",
    "    \n",
    "    xvalid = train_copy[train['kfold'] == fold][useful_cols]\n",
    "    yvalid = train_copy[train['kfold'] == fold][target]\n",
    "\n",
    "    xtest = test_copy[useful_cols]\n",
    "\n",
    "#     model = LGBMClassifier(\n",
    "#         n_estimators=1000,\n",
    "#         random_state=42,\n",
    "#     )\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=7000,\n",
    "        random_state=42,\n",
    "        **xgb_params\n",
    "    )\n",
    "    xgb_model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=False)\n",
    "    \n",
    "    preds_valid = xgb_model.predict(xvalid)\n",
    "    test_preds = xgb_model.predict(xtest)\n",
    "    final_predictions.append(test_preds)\n",
    "    score = roc_auc_score(yvalid, preds_valid)\n",
    "    scores.append(score)\n",
    "    print(fold, score)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))\n",
    "\n",
    "# [6e843db7-46d0-4d55-9b7f-225d5b022227] 0.7920062707849765 0.000711474747631675 - xgb - new ds - optuna minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0e5a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "\n",
    "submission = pd.read_csv('./data/SampleSubmission.csv')\n",
    "submission.CHURN = preds\n",
    "# [befcaa18-90a9-4a54-a901-642c6bf6fe5c] xgb - new ds - optuna\n",
    "submission.to_csv('./data/submission-xgb-6e843db7-46d0-4d55-9b7f-225d5b022227.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16f12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
