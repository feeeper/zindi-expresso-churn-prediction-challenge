{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72b40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dc7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/Train.zip')\n",
    "df_test = pd.read_csv('./data/Test.zip')\n",
    "\n",
    "automl = TabularAutoML(\n",
    "    task = Task(\n",
    "        name = 'binary',\n",
    "        metric = lambda y_true, y_pred: f1_score(y_true, (y_pred > 0.5)*1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9aaf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:20] Stdout logging level is INFO2.\n",
      "[17:50:20] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[17:50:20] Task: binary\n",
      "\n",
      "[17:50:20] Start automl preset with listed constraints:\n",
      "[17:50:20] - time: 3600.00 seconds\n",
      "[17:50:20] - CPU: 4 cores\n",
      "[17:50:20] - memory: 16 GB\n",
      "\n",
      "[17:50:20] \u001b[1mTrain data shape: (2154048, 19)\u001b[0m\n",
      "\n",
      "[17:50:28] Layer \u001b[1m1\u001b[0m train process start. Time left 3592.09 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\projects\\zindi-expresso-churn-prediction-challenge\\venv\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:997: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:51:56] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[17:51:56] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[17:52:30] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[17:53:06] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[17:53:43] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[17:54:23] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m =====\n",
      "[17:55:00] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.6884711963532367\u001b[0m\n",
      "[17:55:00] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[17:55:00] Time left 3320.14 secs\n",
      "\n",
      "[17:56:50] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[17:57:24] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[17:57:24] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[17:59:20] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m =====\n",
      "[18:01:24] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[18:01:24] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.6704853400430156\u001b[0m\n",
      "[18:01:24] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[18:01:24] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 1.00 secs\n",
      "[18:03:22] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[18:03:22] The set of hyperparameters \u001b[1m{'feature_fraction': 0.6872700594236812, 'num_leaves': 244, 'bagging_fraction': 0.8659969709057025, 'min_sum_hessian_in_leaf': 0.24810409748678125, 'reg_alpha': 2.5361081166471375e-07, 'reg_lambda': 2.5348407664333426e-07}\u001b[0m\n",
      " achieve 0.6691 AutoML Metric\n",
      "[18:03:22] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[18:03:22] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m =====\n",
      "[18:05:20] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m =====\n",
      "[18:07:17] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m =====\n",
      "[18:09:13] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m =====\n",
      "[18:11:12] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m =====\n",
      "[18:13:20] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m0.6707740835898349\u001b[0m\n",
      "[18:13:20] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[18:13:20] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[18:13:20] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
      "[18:20:48] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m =====\n",
      "[18:30:55] Time limit exceeded after calculating fold 1\n",
      "\n",
      "[18:30:55] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.6696625494495702\u001b[0m\n",
      "[18:30:55] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[18:30:55] Time left 1164.42 secs\n",
      "\n",
      "[18:30:55] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[18:30:55] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[18:30:56] Blending: optimization starts with equal weights and score \u001b[1m0.6821232176748261\u001b[0m\n",
      "[18:31:25] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.6885919811811247\u001b[0m, weights = \u001b[1m[0.90518254 0.         0.09481741 0.        ]\u001b[0m\n",
      "[18:31:54] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.6885919811811247\u001b[0m, weights = \u001b[1m[0.90518254 0.         0.09481741 0.        ]\u001b[0m\n",
      "[18:31:54] Blending: no score update. Terminated\n",
      "\n",
      "[18:31:54] \u001b[1mAutoml preset training completed in 2494.64 seconds\u001b[0m\n",
      "\n",
      "[18:31:54] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.90518 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.09482 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred = automl.fit_predict(\n",
    "    df_train,\n",
    "    roles={'target': 'CHURN', 'drop': ['MRG']},\n",
    "    verbose=2)\n",
    "\n",
    "test_pred = automl.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688e2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['CHURN'] = test_pred.data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f20f1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['user_id', 'CHURN']].to_csv('./data/lama.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
